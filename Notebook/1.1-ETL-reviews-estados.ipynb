{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __ETL__ _(Extract, Transform, Load)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "Este notebook se enfoca en el proceso de **ETL** utilizando datos extraídos de las plataformas Yelp y Google Maps. Este proceso implica una _extracccion,transformación y carga_ de los datos con el objetivo de prepararlos para análisis posteriores. Este paso es crucial en cualquier proyecto de ciencia de datos para garantizar la calidad y utilidad de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuraciones Globales e Importaciones\n",
    "\n",
    "En esta sección, se instalan e importan todas las librerías y/o módulos necesarios para el proceso ETL (Extract, Transform, Load) y se establecen configuraciones globales de ser requerido. Se utilizan las siguientes librerías y herramientas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # Se utiliza para gestionar las advertencias y mantener el código limpio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # Proporciona funciones para interactuar con el sistema operativo.\n",
    "import requests # Se utiliza para realizar solicitudes HTTP.\n",
    "import pandas as pd # Una librería de análisis de datos.\n",
    "import seaborn as sns #S e utiliza para la visualización de datos.\n",
    "import pyspark.pandas as ps # Proporciona una interfaz para trabajar con datos en Spark utilizando el formato de DataFrame de pandas.\n",
    "import json # Se utiliza para trabajar con datos en formato JSON.\n",
    "from pyspark.sql import SparkSession # Se utiliza para crear una instancia de SparkSession, que es la entrada principal para trabajar con Spark SQL.\n",
    "from pyspark.sql import functions as F #  Proporciona funciones para trabajar con datos en Spark DataFrame.\n",
    "from pyspark.sql.functions import array_contains # Esta función se utiliza para filtrar los datos basados en la presencia de un valor en un array.\n",
    "from pyspark.sql.functions import sum, col # Se utiliza para acceder a una columna en un DataFrame de Spark.\n",
    "from pyspark.sql.functions import split, substring, concat_ws\n",
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una sesión de Spark\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"ETL-reviews-estados\")\\\n",
    "        .config('spark.ui.port', '4050')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REVIEWS-ESTADOS\n",
    "\n",
    "**Dataset:** NEW YORK y CALIFORNIA\n",
    "\n",
    "DECLARACIÓN DE LA RUTA DE LOS DATA SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta del directorio que contiene los archivos de reseñas de Nueva York\n",
    "ruta_reviews_ny = \"C:/Users/Usuario/Desktop/Proyecto Final/review-New_York\"\n",
    "ruta_reviews_cf = \"C:/Users/Usuario/Desktop/Proyecto Final/reviews-California\"\n",
    "\n",
    "# Lista para almacenar los DataFrames de reseñas de Nueva York y California\n",
    "df_NY = []\n",
    "df_CA = []\n",
    "\n",
    "# Inicialización de variables\n",
    "i = 1\n",
    "bandera = True\n",
    "\n",
    "# Bucle para leer los archivos JSON de reseñas de Nueva York\n",
    "while bandera:\n",
    "    try:\n",
    "        # Lectura del archivo JSON utilizando Spark y agregándolo a la lista\n",
    "        archivo = spark.read.json(f\"{ruta_reviews_ny}/{i}.json\")\n",
    "        df_NY.append(archivo)\n",
    "        i += 1\n",
    "    except:\n",
    "        # Finalización del bucle si no hay más archivos\n",
    "        bandera = False\n",
    "\n",
    "# Inicialización de variables\n",
    "i = 1\n",
    "bandera = True\n",
    "\n",
    "# Bucle para leer los archivos JSON de reseñas de California\n",
    "while bandera:\n",
    "    try:\n",
    "        # Lectura del archivo JSON utilizando Spark y agregándolo a la lista\n",
    "        archivo = spark.read.json(f\"{ruta_reviews_cf}/{i}.json\")\n",
    "        df_CA.append(archivo)\n",
    "        i += 1\n",
    "    except:\n",
    "        # Finalización del bucle si no hay más archivos\n",
    "        bandera = False\n",
    "\n",
    "# DataFrame final para almacenar las reseñas de Nueva York combinadas\n",
    "df_final_reviews_ny = df_NY[0]\n",
    "# Unión de los DataFrames de reseñas de Nueva York\n",
    "for dataframe in df_NY[1:]:\n",
    "    df_final_reviews_ny = df_final_reviews_ny.unionByName(dataframe)\n",
    "\n",
    "# DataFrame final para almacenar las reseñas de California combinadas\n",
    "df_final_reviews_ca = df_CA[0]\n",
    "# Unión de los DataFrames de reseñas de California\n",
    "for dataframe in df_CA[1:]:\n",
    "    df_final_reviews_ca = df_final_reviews_ca.unionByName(dataframe)\n",
    "\n",
    "# Sobrescribir df_NY y df_CA con los DataFrames finales combinados\n",
    "df_NY = df_final_reviews_ny\n",
    "df_CA = df_final_reviews_ca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenar los DataFrames \n",
    "df = df_NY.union(df_CA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+------+--------------------+--------------------+-------------+--------------------+\n",
      "|             gmap_id|                name|                pics|rating|                resp|                text|         time|             user_id|\n",
      "+--------------------+--------------------+--------------------+------+--------------------+--------------------+-------------+--------------------+\n",
      "|0x89c25fc9494dce4...|      Alvin Martinez|[{[https://lh5.go...|     5|                NULL|I'm late to posti...|1603494795361|11372210469230823...|\n",
      "|0x89c25fc9494dce4...|     Johnnie Jackson|                NULL|     1|{We pride ourselv...|Very dissatisfied...|1620157037403|10729344149210932...|\n",
      "|0x89c25fc9494dce4...|        Manie Blazer|                NULL|     5|                NULL|Excellent very we...|1597431662039|10037858580181940...|\n",
      "|0x89c25fc9494dce4...|      Fashion Fiinds|                NULL|     5|{Thanks for the a...|Basing my review ...|1543773862044|11499816115301982...|\n",
      "|0x89c25fc9494dce4...|      Andres Rieloff|                NULL|     1|                NULL|Bad! Disorganized...|1597279097718|11717818572842229...|\n",
      "|0x89c25fc9494dce4...|   claribel placeres|                NULL|     1|                NULL|Worse customer ev...|1456098569126|11055512483166433...|\n",
      "|0x89c25fc9494dce4...|        Manie Blazer|                NULL|     5|                NULL|Excellent very we...|1597431662039|10037858580181940...|\n",
      "|0x89c25fc9494dce4...|   claribel placeres|                NULL|     1|                NULL|Worse customer ev...|1456098569126|11055512483166433...|\n",
      "|0x89c25fc9494dce4...|       Mireya Robles|                NULL|     5|{We love our T-Mo...|(Translated by Go...|1614117783581|11027085689719977...|\n",
      "|0x89c25fc9494dce4...|Jose Abimelec “ME...|                NULL|     5|                NULL|(Translated by Go...|1603310536331|11018246194682118...|\n",
      "|0x89c25fc9494dce4...|       Thomas Schmid|                NULL|     4|                NULL|(Translated by Go...|1469775211732|11009832941668217...|\n",
      "|0x89c25fc9494dce4...|           Ramon Coj|                NULL|     4|                NULL|(Translated by Go...|1529458074142|10958366381447636...|\n",
      "|0x89c25fc9494dce4...|       Sol Brillante|                NULL|     5|                NULL|(Translated by Go...|1489859075479|10536279556931633...|\n",
      "|0x89c25fc9494dce4...|      Jose l. Orduna|                NULL|     1|                NULL|(Translated by Go...|1530211344356|11675595500793973...|\n",
      "|0x89c25fc9494dce4...|         Juanjose A.|                NULL|     1|                NULL|(Translated by Go...|1460605543817|11407485266563370...|\n",
      "|0x89c25fc9494dce4...|     Oscar Fernandez|                NULL|     1|                NULL|(Translated by Go...|1489106271091|10549910428286469...|\n",
      "|0x89c25fc9494dce4...|      Renato Vanegas|[{[https://lh5.go...|     5|{(Translated by G...|(Translated by Go...|1543452106697|10650755205352997...|\n",
      "|0x89c25fc9494dce4...|    Tatiana Martinez|                NULL|     1|{We are sad to he...|(Translated by Go...|1553625245873|10776215246482482...|\n",
      "|0x89c25fc9494dce4...|    Rafael Bacuilima|                NULL|     5|                NULL|                NULL|1592264617160|10492724686594474...|\n",
      "|0x89c25fc9494dce4...|      Hamzeh Emriesh|                NULL|     5|                NULL|                NULL|1611798876088|11610275551580018...|\n",
      "+--------------------+--------------------+--------------------+------+--------------------+--------------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Mostrar el DATAFRAME\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5400000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Contamos filas\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuenta el número de nulos en cada columna\n",
    "\n",
    "def conteo_nulos(dataframe):\n",
    "  conteo_nulos_por_columna = dataframe.agg(*[sum(col(c).isNull().cast(\"int\")).alias(c) for c in dataframe.columns])\n",
    "\n",
    "  # Muestra el resultado\n",
    "  conteo_nulos_por_columna.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-------+------+-------+-------+----+-------+\n",
      "|gmap_id|name|   pics|rating|   resp|   text|time|user_id|\n",
      "+-------+----+-------+------+-------+-------+----+-------+\n",
      "|      0|   0|5199054|     0|4890834|2334557|   0|      0|\n",
      "+-------+----+-------+------+-------+-------+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conteo_nulos(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de filas después de eliminar duplicados: 5265421\n"
     ]
    }
   ],
   "source": [
    "# Eliminar duplicados basándote en todas las columnas\n",
    "df_no_duplicates = df.dropDuplicates()\n",
    "\n",
    "# Contar las filas después de eliminar duplicados\n",
    "print(\"Número de filas después de eliminar duplicados:\", df_no_duplicates.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procedio a buscar la cantidades de nulos que teniamos como tambien los duplicados. El _total de duplicados fue: 134,579_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente paso procederemos a _eliminar las columnas pics y resp_ porque contienen más del 90% de los datos nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de columnas a eliminar \n",
    "columnas_a_eliminar = ['pics', 'resp'] \n",
    "# Elimina las columnas especificadas \n",
    "df = df.select([columna for columna in df.columns if columna not in columnas_a_eliminar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+--------------------+-------------+--------------------+\n",
      "|             gmap_id|                name|rating|                text|         time|             user_id|\n",
      "+--------------------+--------------------+------+--------------------+-------------+--------------------+\n",
      "|0x89c25fc9494dce4...|      Alvin Martinez|     5|I'm late to posti...|1603494795361|11372210469230823...|\n",
      "|0x89c25fc9494dce4...|     Johnnie Jackson|     1|Very dissatisfied...|1620157037403|10729344149210932...|\n",
      "|0x89c25fc9494dce4...|        Manie Blazer|     5|Excellent very we...|1597431662039|10037858580181940...|\n",
      "|0x89c25fc9494dce4...|      Fashion Fiinds|     5|Basing my review ...|1543773862044|11499816115301982...|\n",
      "|0x89c25fc9494dce4...|      Andres Rieloff|     1|Bad! Disorganized...|1597279097718|11717818572842229...|\n",
      "|0x89c25fc9494dce4...|   claribel placeres|     1|Worse customer ev...|1456098569126|11055512483166433...|\n",
      "|0x89c25fc9494dce4...|        Manie Blazer|     5|Excellent very we...|1597431662039|10037858580181940...|\n",
      "|0x89c25fc9494dce4...|   claribel placeres|     1|Worse customer ev...|1456098569126|11055512483166433...|\n",
      "|0x89c25fc9494dce4...|       Mireya Robles|     5|(Translated by Go...|1614117783581|11027085689719977...|\n",
      "|0x89c25fc9494dce4...|Jose Abimelec “ME...|     5|(Translated by Go...|1603310536331|11018246194682118...|\n",
      "|0x89c25fc9494dce4...|       Thomas Schmid|     4|(Translated by Go...|1469775211732|11009832941668217...|\n",
      "|0x89c25fc9494dce4...|           Ramon Coj|     4|(Translated by Go...|1529458074142|10958366381447636...|\n",
      "|0x89c25fc9494dce4...|       Sol Brillante|     5|(Translated by Go...|1489859075479|10536279556931633...|\n",
      "|0x89c25fc9494dce4...|      Jose l. Orduna|     1|(Translated by Go...|1530211344356|11675595500793973...|\n",
      "|0x89c25fc9494dce4...|         Juanjose A.|     1|(Translated by Go...|1460605543817|11407485266563370...|\n",
      "|0x89c25fc9494dce4...|     Oscar Fernandez|     1|(Translated by Go...|1489106271091|10549910428286469...|\n",
      "|0x89c25fc9494dce4...|      Renato Vanegas|     5|(Translated by Go...|1543452106697|10650755205352997...|\n",
      "|0x89c25fc9494dce4...|    Tatiana Martinez|     1|(Translated by Go...|1553625245873|10776215246482482...|\n",
      "|0x89c25fc9494dce4...|    Rafael Bacuilima|     5|                NULL|1592264617160|10492724686594474...|\n",
      "|0x89c25fc9494dce4...|      Hamzeh Emriesh|     5|                NULL|1611798876088|11610275551580018...|\n",
      "+--------------------+--------------------+------+--------------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforma la columna \"time\" de milisegundos a timestamp.\n",
    "df = df.withColumn(\"time\", (col(\"time\") / 1000).cast('timestamp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecciona las columnas 'user_id' y 'name' para crear la tabla user. \n",
    "user = df.select('user_id', 'name').dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             user_id|                name|\n",
      "+--------------------+--------------------+\n",
      "|11409232417054963...|         Nataly Diaz|\n",
      "|11821652550697589...|          Miss Arrow|\n",
      "|10447435380083412...|           Michael G|\n",
      "|10076816597589615...|    FAITHFUL SaviOUR|\n",
      "|11053794423377500...|Rafaelina. Rivera...|\n",
      "|11226579810509041...|         Sam Smithen|\n",
      "|10746510942947335...|    Shannon Kauderer|\n",
      "|10566103281342686...|              Sal P.|\n",
      "|11168740463737060...|      Matthew Hanson|\n",
      "|11501759531064434...|       Amanda Rivera|\n",
      "|11827956310180636...|            Daniel S|\n",
      "|10133343740820331...|          Jae Soulja|\n",
      "|11385457655859633...|       Julie Piracha|\n",
      "|10348155874117546...|    Stephen Ferrigno|\n",
      "|11146568800797908...|      Michelle Zhang|\n",
      "|11792012147987443...|Colleen Kelly-Sle...|\n",
      "|11799064012349328...|           Britt Liz|\n",
      "|11359111095002785...|       Emily Karsten|\n",
      "|10052668183086772...|   Sanjoy Purkaystha|\n",
      "|11068973666000718...|        Patrick Wynn|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimina la columna 'name' del DataFrame original en PySpark. \n",
    "df = df.drop('name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de nuestro archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta al archivo Parquet local \n",
    "file_path = 'C:/Users/Usuario/Desktop/Proyecto Final/PF_Google_yelp_Map/Notebook/reviews-estados-limpios.parquet' \n",
    "# Escribe el DataFrame a un archivo Parquet localmente \n",
    "df.write.parquet(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
